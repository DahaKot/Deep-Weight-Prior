{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xYI7HXnHk0S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsW8AKUuHk0l"
   },
   "outputs": [],
   "source": [
    "#better change these parameters\n",
    "N_NETWORKS = 100\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS   = 200\n",
    "STAT_STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png\n",
    "!rm ./notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png\n",
    "!rm ./notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png\n",
    "!rm ./notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png\n",
    "!rm ./notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55FMLtDJHk0u"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = './notMNIST_large'\n",
    "MODEL_STORE_PATH = './models_notMNIST/model_'\n",
    "model_paths = []\n",
    "for i in range(N_NETWORKS):\n",
    "    model_paths.append(MODEL_STORE_PATH + str(i) + '.pt')\n",
    "    file = open(model_paths[i], 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFT1DkRTHk07"
   },
   "outputs": [],
   "source": [
    "#transform data to tensor and normalize (values state for MNIST!)\n",
    "trans = tt.Compose([tt.Grayscale(), tt.ToTensor()]) \n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=trans) \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFT1DkRTHk07"
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "\n",
    "# item = iter(train_loader)\n",
    "\n",
    "# i = 0\n",
    "# try:\n",
    "#     while 1:\n",
    "#         print(i)\n",
    "#         i+= 1\n",
    "#         try:\n",
    "#             item.__next__()\n",
    "#         except IOError as e:\n",
    "#             print(\"found broken image\", e)\n",
    "# except StopIteration:\n",
    "#     print(\"All data is allright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hasw1qZ9Hk1E"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOn6BLw1Hk1P"
   },
   "outputs": [],
   "source": [
    "class FConvMNIST(nn.Module):\n",
    "    def __init__(self, seq, clf):\n",
    "        super(FConvMNIST, self).__init__()\n",
    "        self.seq = seq\n",
    "        self.clf = clf\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.clf(self.seq(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2Ocyzk3Hk1b"
   },
   "outputs": [],
   "source": [
    "#architecture of the network copied from the article\n",
    "seq = nn.Sequential(\n",
    "    nn.Conv2d(1, 256, kernel_size=(7, 7), stride=(1, 1)),\n",
    "    nn.LeakyReLU(negative_slope=0.01),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    nn.Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1)),\n",
    "    nn.LeakyReLU(negative_slope=0.01),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    Flatten()\n",
    ")\n",
    "clf = nn.Linear(in_features=4608, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAMGrwDSHk1p"
   },
   "outputs": [],
   "source": [
    "model = FConvMNIST(seq, clf)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuvQlzkZHk12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16535\n",
      "Network 0/100 Epoch 0/200, Step 0/16535, Loss: 2.330111503601074, Accuracy: 3.125%\n",
      "Network 0/100 Epoch 0/200, Step 10/16535, Loss: 1.4021883010864258, Accuracy: 28.40909090909091%\n",
      "Network 0/100 Epoch 0/200, Step 20/16535, Loss: 1.2587320804595947, Accuracy: 40.476190476190474%\n",
      "Network 0/100 Epoch 0/200, Step 30/16535, Loss: 1.145156741142273, Accuracy: 47.07661290322581%\n",
      "Network 0/100 Epoch 0/200, Step 40/16535, Loss: 1.0366491079330444, Accuracy: 51.0670731707317%\n",
      "Network 0/100 Epoch 0/200, Step 50/16535, Loss: 1.0923550128936768, Accuracy: 54.595588235294116%\n",
      "Network 0/100 Epoch 0/200, Step 60/16535, Loss: 0.8272674679756165, Accuracy: 57.120901639344254%\n",
      "Network 0/100 Epoch 0/200, Step 70/16535, Loss: 0.8831173181533813, Accuracy: 59.198943661971825%\n",
      "Network 0/100 Epoch 0/200, Step 80/16535, Loss: 0.8665003776550293, Accuracy: 60.60956790123457%\n",
      "Network 0/100 Epoch 0/200, Step 90/16535, Loss: 0.7365061044692993, Accuracy: 62.019230769230774%\n",
      "Network 0/100 Epoch 0/200, Step 100/16535, Loss: 0.7987658977508545, Accuracy: 63.64480198019802%\n",
      "Network 0/100 Epoch 0/200, Step 110/16535, Loss: 0.6808359622955322, Accuracy: 64.7804054054054%\n",
      "Network 0/100 Epoch 0/200, Step 120/16535, Loss: 0.5414015054702759, Accuracy: 66.29648760330579%\n",
      "Network 0/100 Epoch 0/200, Step 130/16535, Loss: 0.6832267045974731, Accuracy: 67.36641221374046%\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list  = []\n",
    "N_STEPS = len(train_loader)\n",
    "print(N_STEPS)\n",
    "\n",
    "#train network\n",
    "for i in range(N_NETWORKS):\n",
    "    for j in range(N_EPOCHS):\n",
    "        for k, (images, labels) in enumerate(train_loader):\n",
    "            if (images.shape[0] < BATCH_SIZE):\n",
    "                break\n",
    "            #forward\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            #statictics\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            \n",
    "            correct = (predicted == labels).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "\n",
    "            if k % STAT_STEP == 0:\n",
    "                print('Network {}/{} Epoch {}/{}, Step {}/{}, Loss: {}, Accuracy: {}%'\n",
    "                      .format(i, N_NETWORKS, j, N_EPOCHS, k, N_STEPS, loss.item(),\n",
    "                              (sum(acc_list) / len(acc_list)) * 100))\n",
    "                \n",
    "    torch.save(model.state_dict(), model_paths[i])\n",
    "    close(model_paths[i], \"w\")\n",
    "    model = FConvMNIST(seq, clf)\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSV3xGwXHk3Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hundred_networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
