{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xYI7HXnHk0S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as tt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsW8AKUuHk0l"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "N_EPOCHS   = 100\n",
    "DATA_PATH = '../datasets/MNIST/'\n",
    "Z_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55FMLtDJHk0u"
   },
   "outputs": [],
   "source": [
    "#cuda!\n",
    "#test on colab at least one iteration\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, D, d = Z_DIM):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(d, D)\n",
    "        self.decoder = Decoder(d, D)\n",
    "        \n",
    "        #d - for latent space\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        \n",
    "    def count_vlb(self, X, i):\n",
    "        #get parameters of z distribution\n",
    "        mu_z, var_z = self.encoder(X)\n",
    "\n",
    "        #sample latent(mu_z, var_z)\n",
    "        z = Normal(torch.zeros(mu_z.size()), torch.ones(mu_z.size())).sample()\n",
    "        z = mu_z + torch.sqrt(var_z) * nn.Parameter(z, requires_grad=False).to(device)\n",
    "        latent_loss = 0.5 * (var_z + mu_z**2 - torch.log(var_z+1e-12) - 1).mean(dim=0).sum()\n",
    "        \n",
    "        #get parameters of x_hat distribution\n",
    "        mu_x, var_x = self.decoder(z)\n",
    "        normal = Normal(mu_x, torch.sqrt(var_x))\n",
    "        reconstruction_loss = normal.log_prob(X).mean(dim=0).sum()\n",
    "        #count loss\n",
    "        #coefs are to be chosen\n",
    "        return -(reconstruction_loss - latent_loss), latent_loss, reconstruction_loss\n",
    "                   \n",
    "    def generate_samples(self, num_samples):\n",
    "        z = torch.FloatTensor(np.random.normal(0, 1, size=[num_samples, self.d, 1, 1])).to(device)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.d = d\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0)\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Conv2d(64, Z_DIM, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.fc_var = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.seq(X)\n",
    "        \n",
    "        mu = self.fc_mu(X)\n",
    "        var = self.fc_var(X)\n",
    "        \n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.d = d\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.ConvTranspose2d(Z_DIM, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ELU(alpha=1.0)\n",
    "        )\n",
    "        self.fc_mu = nn.Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.fc_var = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.seq(X)\n",
    "        \n",
    "        mu = self.fc_mu(X)\n",
    "        var = self.fc_var(X)\n",
    "        \n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "vae = VAE(49).to(device)\n",
    "vae.load_state_dict(torch.load(\"vae_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFT1DkRTHk07"
   },
   "outputs": [],
   "source": [
    "#transform data to tensor and normalize (values state for MNIST!)\n",
    "trans = tt.Compose([tt.ToTensor()]) \n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True,  transform=trans, download=False) \n",
    "test_dataset  = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hasw1qZ9Hk1E"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOn6BLw1Hk1P"
   },
   "outputs": [],
   "source": [
    "class FConvMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FConvMNIST, self).__init__()\n",
    "        \n",
    "        self.conv1   = nn.Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
    "        self.relu    = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2   = nn.Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.flat    = Flatten()\n",
    "        self.clf     = nn.Linear(in_features=1152, out_features=10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        return self.clf(self.flat(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAMGrwDSHk1p"
   },
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "model = FConvMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 7, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.generate_samples(32)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[[-0.1055, -0.0073,  0.0881, -0.0607, -0.0919, -0.0491, -0.0191],\n",
      "         [-0.0686,  0.0187,  0.1359, -0.0357, -0.0116, -0.0727, -0.0830],\n",
      "         [ 0.0486,  0.0721,  0.0993, -0.0520,  0.0552, -0.0868,  0.0595],\n",
      "         [ 0.1294, -0.0316, -0.0567,  0.0783, -0.0668,  0.1337, -0.0163],\n",
      "         [ 0.1241, -0.0969,  0.1406,  0.1175, -0.0510, -0.1090, -0.1242],\n",
      "         [-0.1406, -0.0586,  0.0172,  0.0268,  0.0182,  0.0703, -0.0112],\n",
      "         [-0.1101, -0.0693,  0.0796,  0.1162,  0.1059, -0.0840, -0.1076]]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "2 tensor([[[ 0.0042,  0.0108,  0.0076,  0.0041, -0.0075, -0.0461, -0.0568],\n",
      "         [-0.0169,  0.0066,  0.0117,  0.0017, -0.0167, -0.0308, -0.0327],\n",
      "         [-0.0444, -0.0162, -0.0105, -0.0222, -0.0379, -0.0249,  0.0471],\n",
      "         [-0.0309, -0.0096, -0.0200, -0.0469, -0.0566, -0.0160,  0.1109],\n",
      "         [-0.0170, -0.0174, -0.0390, -0.0583, -0.0600,  0.0155,  0.1724],\n",
      "         [-0.0335, -0.0341, -0.0416, -0.0534, -0.0354,  0.0220,  0.1721],\n",
      "         [-0.0684, -0.0705, -0.0494, -0.0360,  0.0052,  0.0718,  0.1745]]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"1\", model.conv1.weight[0])\n",
    "model.conv1.weight = nn.Parameter(vae.generate_samples(32)[0])\n",
    "print(\"2\", model.conv1.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f99b0a6aef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC45JREFUeJzt3VuIXYUVxvHvm8lEM4mahkQbMqFJwUpFqLFDSglIm1qNNdQ+9EFBoaXFl1oiFkR9aPG5xdqHUghJWouXIMaAiLeABivUSxJjY0yUEJRMYzqx3jLROpmZ1YfZsWMcenacfTku/j8Ycs7MzllrLt/Zl3P2Xo4IAcipp+0GANSHgAOJEXAgMQIOJEbAgcQIOJAYAQcSI+BAYgQcSGxWHQ/a2z83+uYvqOOhO3KLb8wLt1e7bW3+3HvG2qvd1u989Ng7GvvoeMfqtQS8b/4CLfv5zXU8dEe9o62UlSSN97VXWy0/ufScaK/2nKPtPbtMtPQ7f23L70stxyY6kBgBBxIj4EBiBBxIjIADiRFwIDECDiRGwIHECDiQGAEHEisVcNtrbL9m+4DtW+tuCkA1Ogbcdq+kP0q6UtKFkq61fWHdjQGYuTJr8JWSDkTEwYgYlbRZ0tX1tgWgCmUCvkTSoSn3h4rPAehyZQI+3YmInzk/z/YNtnfY3jH+4fGZdwZgxsoEfEjS0in3ByQdPnWhiFgfEYMRMdjbP7eq/gDMQJmAvyjpfNvLbc+WdI2kh+ttC0AVOl7RJSLGbN8o6QlJvZI2RcTe2jsDMGOlLtkUEY9KerTmXgBUjHeyAYkRcCAxAg4kRsCBxAg4kBgBBxIj4EBiBBxIjIADiRFwILFapotKam3a5dicdupK0scLx1ur7RPtjhc949321hWzR9qbLnr8yy193yV/3azBgcQIOJAYAQcSI+BAYgQcSIyAA4kRcCAxAg4kRsCBxAg4kBgBBxIrM110k+1h26800RCA6pRZg/9F0pqa+wBQg44Bj4hnJL3TQC8AKsY+OJBYZQFnfDDQfSoLOOODge7DJjqQWJmXye6X9HdJF9gesv2z+tsCUIUy88GvbaIRANVjEx1IjIADiRFwIDECDiRGwIHECDiQGAEHEiPgQGIEHEiMgAOJ1TM+uEca629npOuJ+e2N8O09Z7S12hPvntFabUnqP9LeCN/+w/9prfZHC1qaV13yx80aHEiMgAOJEXAgMQIOJEbAgcQIOJAYAQcSI+BAYgQcSIyAA4kRcCCxMtdFX2r7adv7bO+1va6JxgDMXJmTTcYk/Soidtk+S9JO29si4tWaewMwQ2XGB78VEbuK28ck7ZO0pO7GAMzcae2D214maYWk5+toBkC1Sgfc9jxJWyTdFBEfTPP1/40PPs74YKAblAq47T5NhvveiHhoumU+NT54LuODgW5Q5ii6JW2UtC8i7qy/JQBVKbMGXyXpekmrbe8uPn5Qc18AKlBmfPCzktxALwAqxjvZgMQIOJAYAQcSI+BAYgQcSIyAA4kRcCAxAg4kRsCBxAg4kFgt44PDUvTW8cglzGpvjO3Ev9sb4fulPe0+V5/79JHWak+8+c/Wao+s/WYrdSdml1uONTiQGAEHEiPgQGIEHEiMgAOJEXAgMQIOJEbAgcQIOJAYAQcSI+BAYmUGH5xp+wXbLxfjg+9oojEAM1fmZJOPJa2OiJFihNGzth+LiOdq7g3ADJUZfBCSRoq7fcVHe6dsASit7PDBXtu7JQ1L2hYRjA8GvgBKBTwixiPiYkkDklbavujUZaaOD54YYXww0A1O6yh6RLwnabukNdN87ZPxwT3zGB8MdIMyR9EX2Z5f3J4j6TJJ++tuDMDMlTmKvljS3bZ7NfmE8EBEPFJvWwCqUOYo+j8krWigFwAV451sQGIEHEiMgAOJEXAgMQIOJEbAgcQIOJAYAQcSI+BAYgQcSIyAA4nVMh/cIfWcqOORO5v9r1q+pVLmHWqttBbtOtZecUkThw63Vrt34YLWas/+2get1PWZ46WWYw0OJEbAgcQIOJAYAQcSI+BAYgQcSIyAA4kRcCAxAg4kRsCBxEoHvJhP9pJtrokOfEGczhp8naR9dTUCoHplp4sOSLpK0oZ62wFQpbJr8Lsk3SJposZeAFSszPDBtZKGI2Jnh+U+GR88fpzxwUA3KLMGXyXph7bfkLRZ0mrb95y60NTxwb1zGR8MdIOOAY+I2yJiICKWSbpG0lMRcV3tnQGYMV4HBxI7resbRcR2Sdtr6QRA5ViDA4kRcCAxAg4kRsCBxAg4kBgBBxIj4EBiBBxIjIADiRFwIDECDiRWy6zdnhPSnCOu46E7mvN2e9ekmP/q+63Vntjzemu1JxsoN862DkevWN5a7V9fdF8rdW+fU+5vjTU4kBgBBxIj4EBiBBxIjIADiRFwIDECDiRGwIHECDiQGAEHEiPgQGKl3otejC06Jmlc0lhEDNbZFIBqnM7JJt+NiLdr6wRA5dhEBxIrG/CQ9KTtnbZvmG6BqeODxz5ifDDQDcpuoq+KiMO2z5W0zfb+iHhm6gIRsV7SeknqP29pVNwngM+h1Bo8Ig4X/w5L2ippZZ1NAahGx4Dbnmv7rJO3JV0u6ZW6GwMwc2U20c+TtNX2yeXvi4jHa+0KQCU6BjwiDkr6RgO9AKgYL5MBiRFwIDECDiRGwIHECDiQGAEHEiPgQGIEHEiMgAOJEXAgsVrGB0ePNNZfxyN3Nnp2O2OLJen9r5/TWu3RwXZP8Bu5YqS12o9967et1V7eN6+Vur/rKTeumTU4kBgBBxIj4EBiBBxIjIADiRFwIDECDiRGwIHECDiQGAEHEiPgQGKlAm57vu0Hbe+3vc/2t+tuDMDMlT3Z5A+SHo+IH9ueLamlU0kAnI6OAbd9tqRLJf1EkiJiVNJovW0BqEKZTfSvSjoq6c+2X7K9oZhR9imfGh/8IeODgW5QJuCzJF0i6U8RsULScUm3nrpQRKyPiMGIGJzV/5n8A2hBmYAPSRqKiOeL+w9qMvAAulzHgEfEEUmHbF9QfOp7kl6ttSsAlSh7FP2Xku4tjqAflPTT+loCUJVSAY+I3ZIGa+4FQMV4JxuQGAEHEiPgQGIEHEiMgAOJEXAgMQIOJEbAgcQIOJAYAQcSc0RU/6D2UUlvfs7/vlDS2xW2Q21qZ6z9lYhY1GmhWgI+E7Z3REQr73unNrWz1WYTHUiMgAOJdWPA11Ob2tSuRtftgwOoTjeuwQFUpKsCbnuN7ddsH7D9mSu31lh3k+1h2680VXNK7aW2ny4mxuy1va7B2mfafsH2y0XtO5qqPaWH3uJy3I80XPcN23ts77a9o+HajU0K6ppNdNu9kl6X9H1NXsn1RUnXRkTtF3i0famkEUl/jYiL6q53Su3FkhZHxC7bZ0naKelHDX3fljQ3IkZs90l6VtK6iHiu7tpTerhZk5cDOzsi1jZY9w1JgxHR+Ovgtu+W9LeI2HByUlBEvFdHrW5ag6+UdCAiDhbTUzZLurqJwhHxjKR3mqg1Te23ImJXcfuYpH2SljRUOyJipLjbV3w09oxve0DSVZI2NFWzbVMmBW2UJicF1RVuqbsCvkTSoSn3h9TQH3q3sL1M0gpJz///JSut2Wt7t6RhSdumXP++CXdJukXSRIM1TwpJT9reafuGBuuWmhRUlW4KuKf5XHfsPzTA9jxJWyTdFBEfNFU3IsYj4mJJA5JW2m5kF8X2WknDEbGziXrTWBURl0i6UtIvit20JpSaFFSVbgr4kKSlU+4PSDrcUi+NKvZ/t0i6NyIeaqOHYjNxu6Q1DZVcJemHxb7wZkmrbd/TUG1FxOHi32FJWzW5i9iERicFdVPAX5R0vu3lxYGHayQ93HJPtSsOdG2UtC8i7my49iLb84vbcyRdJml/E7Uj4raIGIiIZZr8XT8VEdc1Udv23OKAporN48slNfIKStOTgspONqldRIzZvlHSE5J6JW2KiL1N1LZ9v6TvSFpoe0jSbyJiYxO1Nbkmu17SnmJfWJJuj4hHG6i9WNLdxSsYPZIeiIhGX65qyXmStk4+t2qWpPsi4vEG6zc2KahrXiYDUL1u2kQHUDECDiRGwIHECDiQGAEHEiPgQGIEHEiMgAOJ/Re32AQCwkgrRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu_x = model.conv1.weight.detach().cpu().numpy()\n",
    "plt.imshow(mu_x[4].reshape((7, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_acc = []\n",
    "vanilla_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuvQlzkZHk12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "samples  64\n",
      "samples  128\n",
      "samples  192\n",
      "samples  256\n",
      "samples  320\n",
      "samples  384\n",
      "samples  448\n",
      "samples  512\n",
      "samples  576\n",
      "samples  640\n",
      "samples  704\n",
      "samples  768\n",
      "samples  832\n",
      "samples  896\n",
      "samples  960\n",
      "samples  1024\n",
      "samples  1088\n",
      "samples  1152\n",
      "samples  1216\n",
      "samples  1280\n",
      "samples  1344\n",
      "samples  1408\n",
      "samples  1472\n",
      "samples  1536\n",
      "samples  1600\n",
      "samples  1664\n",
      "samples  1728\n",
      "samples  1792\n",
      "samples  1856\n",
      "samples  1920\n",
      "samples  1984\n",
      "samples  2048\n",
      "samples  2112\n",
      "samples  2176\n",
      "samples  2240\n",
      "samples  2304\n",
      "samples  2368\n",
      "samples  2432\n",
      "samples  2496\n",
      "samples  2560\n",
      "samples  2624\n",
      "samples  2688\n",
      "samples  2752\n",
      "samples  2816\n",
      "samples  2880\n",
      "samples  2944\n",
      "samples  3008\n",
      "samples  3072\n",
      "samples  3136\n",
      "samples  3200\n",
      "samples  3264\n",
      "samples  3328\n",
      "samples  3392\n",
      "samples  3456\n",
      "samples  3520\n",
      "samples  3584\n",
      "samples  3648\n",
      "samples  3712\n",
      "samples  3776\n",
      "samples  3840\n",
      "samples  3904\n",
      "samples  3968\n",
      "samples  4032\n",
      "samples  4096\n",
      "samples  4160\n",
      "samples  4224\n",
      "samples  4288\n",
      "samples  4352\n",
      "samples  4416\n",
      "samples  4480\n",
      "samples  4544\n",
      "samples  4608\n",
      "samples  4672\n",
      "samples  4736\n",
      "samples  4800\n",
      "samples  4864\n",
      "samples  4928\n",
      "samples  4992\n",
      "samples  5056\n",
      "samples  5120\n",
      "samples  5184\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list  = []\n",
    "test_acc_list = []\n",
    "\n",
    "N_STEPS = len(train_loader)\n",
    "print(N_STEPS)\n",
    "\n",
    "viewed_batches = 0\n",
    "#train network\n",
    "for j in range(N_EPOCHS):\n",
    "    for k, (images, labels) in enumerate(train_loader):\n",
    "        if (images.shape[0] < BATCH_SIZE):\n",
    "                break\n",
    "        #forward\n",
    "        labels = labels.to(device)\n",
    "        pred = model(images.to(device))\n",
    "        loss = criterion(pred, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        viewed_batches += 1\n",
    "        \n",
    "        if viewed_batches % 1 == 0:\n",
    "            print(\"samples \", viewed_batches * BATCH_SIZE)\n",
    "\n",
    "        #statictics\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        test_acc = []\n",
    "        for i, (test_images, test_labels) in enumerate(test_loader):\n",
    "            if (test_images.shape[0] < BATCH_SIZE):\n",
    "                break\n",
    "            test_labels = test_labels.to(device)\n",
    "            pred = model(test_images.to(device))\n",
    "\n",
    "            #statictics\n",
    "            total = test_labels.size(0)\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "\n",
    "            correct = (predicted == test_labels).sum().item()\n",
    "            test_acc.append(correct / total)\n",
    "\n",
    "        test_acc_list.append(np.mean(test_acc))\n",
    "        \n",
    "        if viewed_batches == 100:\n",
    "            break\n",
    "    if viewed_batches == 100:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot([i for i in range(len(acc_list))], acc_list, label = 'train_accuracy')\n",
    "plt.plot([i for i in range(len(test_acc_list))], test_acc_list, label = 'test_accuracy')\n",
    "\n",
    "vae_acc.append(test_acc_list)\n",
    "\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FConvMNIST().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_loss_list = []\n",
    "vanilla_acc_list  = []\n",
    "vanilla_test_acc_list = []\n",
    "\n",
    "N_STEPS = len(train_loader)\n",
    "print(N_STEPS)\n",
    "viewed_batches = 0\n",
    "#train network\n",
    "for j in range(N_EPOCHS):\n",
    "    for k, (images, labels) in enumerate(train_loader):\n",
    "        if (images.shape[0] < BATCH_SIZE):\n",
    "                break\n",
    "        #forward\n",
    "        labels = labels.to(device)\n",
    "        pred = model(images.to(device))\n",
    "        loss = criterion(pred, labels)\n",
    "        vanilla_loss_list.append(loss.item())\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        viewed_batches += 1\n",
    "        \n",
    "        if viewed_batches % 1 == 0:\n",
    "            print(\"samples \", viewed_batches * BATCH_SIZE)\n",
    "\n",
    "        #statictics\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        vanilla_acc_list.append(correct / total)\n",
    "\n",
    "        vanilla_test = []\n",
    "        for i, (test_images, test_labels) in enumerate(test_loader):\n",
    "            if (i > 20 or test_images.shape[0] < BATCH_SIZE):\n",
    "                break\n",
    "            test_labels = test_labels.to(device)\n",
    "            pred = model(test_images.to(device))\n",
    "            optimizer.step()\n",
    "\n",
    "            #statictics\n",
    "            total = test_labels.size(0)\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "\n",
    "            correct = (predicted == test_labels).sum().item()\n",
    "            vanilla_test.append(correct / total)\n",
    "\n",
    "        vanilla_test_acc_list.append(np.mean(vanilla_test))\n",
    "        \n",
    "        if viewed_batches == 100:\n",
    "            break\n",
    "    if viewed_batches == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(test_acc_list, label = 'vae_test_acc')\n",
    "plt.plot(vanilla_test_acc_list, label = 'xav_test_acc')\n",
    "\n",
    "vanilla_acc.append(vanilla_test_acc_list)\n",
    "\n",
    "plt.xlabel('n samples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vae_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hundred_networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
